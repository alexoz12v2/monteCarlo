\label{chapter2}
\chapter{Fotometria, Colore e Camera}
\section{Fotometria}
L'occhio umano non \`e equamente sensibile a tutte le lunghezza d'onda della luce visibile, la quale \`e un sottoinsieme dello spettro elettromagnetico
corrispondente all'intervallo $[360,830] si{nm}$, come accennato nel primo capitolo \ref{chapter1}. Dunque, si introduce
\begin{definitionS}
	La \textit{Fotometria} \`e la scienza che studia la misurazione della luce in termini della \textit{intensit\`a percepita} dall'occhio umano.
\end{definitionS}
Tale definizione permette di capire che la fotometria si distingue dalla radiometria in quanto, data una sorgente luminosa $\Sigma$ con una certa
SPD, lo scopo \`e pesarne il contributo di ciascuna lunghezza d'onda secondo la percezione umana.\par
L'occhio umano possiede due tipi di fotorecettori: \textit{coni} e \textit{bastoncini}
\begin{itemize}[topsep=0pt, noitemsep]
	\item[] I \textit{coni} sono responsabili per la percezione in ambienti illuminati, detta \textit{visione fotopica}
	\item[] I \textit{bastoncini} sono responsabili per la percezione in ambienti bui, detta \textit{visione scotopica}
\end{itemize}
A seguito di esperimenti condotti nel 1931 dalla CIE, \`e stato associato un modello di risposta spettrale standard, e sono chiamate 
\textit{Efficacia Luminosa Spettrale Fotopica} $V_p(\lambda)$ e \textit{Efficacia Luminosa Spettrale Scotopica} $V_s(\lambda)$.\par
Tali funzioni, in ciascuna lunghezza d'onda, rappresentano un peso $\in[0,1]$, il quale pu\`o essere integrato con una grandezza radiometrica spettrale
per ottenere l'analoga grandezza fotometrica.\footnote{Si noti che tutte le formule viste finora per le grandezze radiometriche, come la relazione 
tra radianza e etendue \ref{chapter1:basicRadiance}, o le relazioni tra grandezze radiometriche}
\begin{align} \label{chapter2:photometricQuantities}
	\text{\Gls{Energia Luminosa} } Q_v &= K\int_\Lambda Q_{e,\lambda}(\lambda)V(\lambda)\mathrm{d}\lambda\\
	\text{\Gls{Flusso Luminoso} } \Phi_v &= K\int_\Lambda\Phi_{e,\lambda}(t, \lambda)V(\lambda)\mathrm{d}\lambda\\
	\text{\Gls{Illuminanza}|\Gls{Emittanza Luminosa} } E_v|M_v &= K\int_\Lambda E_{e,\lambda}|M_{e,\lambda}(\vec{p},\lambda)V(\lambda)\mathrm{d}\lambda\\
	\text{\Gls{Intensita Luminosa} } I_v &= K\int_\Lambda I_{e,\Omega,\lambda}(\hat{\omega},\lambda)V(\lambda)\mathrm{d}\lambda\\
	\text{\Gls{Luminanza}\footnotemark{} } Y &= K\int_\Lambda L_{e,\Omega,\lambda}(\vec{p},\hat{\omega},\lambda)V(\lambda)\mathrm{d}\lambda
\end{align}
dove $\Lambda = [380, 830] \si{nm}$, cio\`e intervallo nel quale le funzioni $V(\lambda) \neq 0$\\
\footnotetext{Nota come piuttosto che seguire la nomenclatura ISO per la Luminanza $L_v$ si \`e scelto di adottare $Y$. I motivi appariranno chiari 
nei cenni sulla colorimetria \ref{chapter2:section:colorimetry}}
Nota che non si \`e indicato quale funzione di efficacia luminosa si sta utilizzando, in quanto le formule sono universalmente valide. Le costanti,
invece, che rappresentano l'\textit{Efficacia Luminosa di una radiazione} cambiano valore:
\begin{itemize}[topsep=0pt, noitemsep]
	\item[] $K = 683.002\,\si{lm/W}$ Efficacia luminosa fotopica 
	\item[] $K \approx 1700\,\si{lm/W}$ Efficacia luminosa scotopica
\end{itemize}
Tali costanti rappresentano l'efficacia luminosa, cio\`e il fattore di conversione \mbox{$\si{W}\rightarrow\si{lm}$} per una radiazione monocromatica a
lunghezza d'onda $555 \si{nm}$.\par
\subsection{fotometria e sistemi di rendering}
La fotometria, nel contesto di Physically based Rendering, \`e utile per la modellazione di sorgenti luminose, per specificare parametri come 
intensit\`a luminosa, light falloff, e cos\`i via.\par
Includendo parametri fotometrici si pu\`o ottenere una descrizione pi\`u vicina all'uomo della percezione della luce mantenendone il rigore fisico.
\section{Colorimetria}\label{chapter2:section:colorimetry}
Il colore \`e un fenomeno la cui origine \`e sia fisica, determinato dalla distribuzione nelle lunghezze d'onda della radianza spettrale, sia 
psicobiologica, in quanto la sua percezione \`e artefatto dell'interpretazione di tali lunghezze d'onda nella corteccia cerebrale posteriore.\par
Ci\`o suggerisce che il colore \textit{non \`e una propriet\`a degli oggetti}, ma della luce da essi riflessa e dunque dipendente alle lunghezze d'onda
contenute nella radiazione incidente.\par
Nonostante specificare un colore sottoforma di spettro di frequenza del flusso radiante sia la modalit\`a pi\`u fedele e ottimale, essa
\begin{itemize}[topsep=0pt, noitemsep]
	\item Non \`e una interfaccia human-friendly e orientata alla percezione
	\item Non corrisponde alla rappresentazione utilizzata dai display odierni
\end{itemize}
Dunque, nel fornire un'interfaccia per un sistema di rendering, bisogna implementare la possibilit\`a di specificare colore in modo intuitivo, 
convertire tale specifica in uno spettro di frequenze per effettuare la computazione, e convertire nella rappresentazione nativa per mostrare a schermo
i risultati.\par
\begin{definitionS}
	Risulta dunque conveniente specificare delle \textit{funzioni di base} per poter costituire uno spazio vettoriale tramite il quale, specificandone
	i componenti, si specifica un determinato colore. Tale spazio vettoriale \`e chiamato \textit{Color Space}.
\end{definitionS}
\begin{figure}
	\centering
	\includegraphics[width=0.4\textwidth]{../assets/chapter2_colorimetry_cones.png}
	\includesvg[width=0.55\textwidth, inkscapelatex=false]{../assets/chapter2_colorimetry_RGB_color_matching_functions.svg}
	\label{chapter2:figure:coneSpectralSensitivity}
	\label{chapter2:figure:RGBColorMatchingFunctions}
	\caption{\textit{A destra}, Sensibilit\`a spettrali per i tre tipi di coni dell'occhio umano. \textit{A sinistra}, color matching functions del 
			CIE 1931 RGB Color Space}
\end{figure}
Un \textit{color stimulus} pu\`o essere ricostruito dalla combinazione lineare da tre stimoli (distribuzioni spettrali) di base indipendenti 
[\cite{color}], in quanto dei fotorecettori umani che distinguono il colore, i coni, ce ne sono di \textit{tre tipi} (vedi figura 
\ref{chapter2:figure:coneSpectralSensitivity}). Tale mappa da SPD a \textit{tristimulus values} non \`e biunivoca, infatti, nei diversi color 
spaces che definiamo, esistono valori tristimolo che possono risultare dalla scomposizione di diverse SPDs. Tali colori 
sono detti \textit{metameri}.\par 
Dunque l'obiettivo \`e quello di ottenere distribuzioni spettrali $\langle R\rangle, \langle G\rangle, \langle B\rangle$ tali che ogni colore 
possa essere rappresentato come
\begin{equation}\label{chapter2:colorimetry:color}
	C = R\langle R\rangle + G\langle G\rangle + B\langle B\rangle
\end{equation}
tale compito non \`e scontato in quanto, da come si osserva in figura \ref{chapter2:figure:coneSpectralSensitivity}, quasi tutte le lunghezza d'onda 
stimolano con diversa intensit\`a almeno due tipi di coni. Dunque, nel 1931, fu derivato il CIE 1931 RGB Trichromatic system, cio\`e una terna di 
color matching functions $\bar{r}(\lambda)$, $\bar{g}(\lambda)$, $\bar{b}(\lambda)$, derivate compiendo dei test con un 
\textit{osservatore standard colorimetrico}, cio\`e limitando il field of view dei soggetti testati a $2^o$ dentro la fovea, per eliminare la 
variabilit\`a di percezione causata dal field of view dell'osservatore. Tali soggetti sono stati esposti a luci monocromatiche, i.e. i colori primari 
scelti, alle frequenze $700\,\si{nm}$ (rosso), $546.1\,\si{nm}$ (verde), $435.8\,\si{nm}$ (blu). L'intensit\`a di queste tre sorgenti primarie erano
tali che, se addizionate, restituiscono uno spettro costante la cui intensit\`a \`e pari alla Luminanza della sorgente complessiva 
\begin{align}
	R &= \int_\Lambda \bar{r}(\lambda)L_{e,\Omega,\lambda}(\lambda)\mathrm{d}\lambda\\
	G &= \int_\Lambda \bar{g}(\lambda)L_{e,\Omega,\lambda}(\lambda)\mathrm{d}\lambda\\
	B &= \int_\Lambda \bar{b}(\lambda)L_{e,\Omega,\lambda}(\lambda)\mathrm{d}\lambda\\
	L_v &= 1.0000 R + 4.5907 G + 0.0601 B\;[\si{cd/m^2}]
\end{align}
Le tre color matching functions (CMFs) cos\`i 
ottenute sono mostrate in figura \ref{chapter2:figure:RGBColorMatchingFunctions}.\par
Ai tempi della standardizzazione di un sistema tricromatico, in assenza di computers, risultava complicato fare calcoli con queste tre CMFs per via dei
lobi negativi. Dunque, sempre nel 1931, fu standardizzato il \textit{CIE 1931 XYZ Color System}, nel quale sono stati scelti tre 
\textit{primari immaginari}\footnote{non percepibili all'occhio umano} a partire dai primari del sistema CIE RGB 1931 in modo tale che
\begin{itemize}[topsep=0pt, noitemsep]
	\item uno spettro costante dia luogo a componenti $X = Y = Z$
	\item la componente $Y$ sia la luminanza della radiazione $Y = L_v$
	\item la CMF $\bar{y}$ sia uguale alla funzione di efficacia luminosa fotopica $\bar{y}(\lambda) = V_p(\lambda)$
	\item $Z$ \`e quasi uguale al blu di CIE RGB
	\item $X$ \`e un mix delle tre CMFs di CIE RGB
\end{itemize}
Dai requisiti la seguente trasformazione lineare \mbox{CIE RGB 1931 $\rightarrow$ CIE XYZ 1931} \`e stata ricavata
\begin{equation}\label{chapter2:colorimetry:RGB2XYZ}
	\begin{bmatrix}
		R \\ G \\ B
	\end{bmatrix}
	=
	\begin{bmatrix}
		2.768892 & 1.751748 & 1.130160 \\
		1.000000 & 4.590700 & 0.060100 \\
		0 & 0.056508 & 5.594292
	\end{bmatrix}
	\begin{bmatrix}
		X \\ Y \\ Z
	\end{bmatrix}
\end{equation}
La quale \`e anche valida per trasformare una CMF del sistema CIE RGB in una CMF del sistema CIE XYZ 1931.\par
Per passare da radianza spettrale ai tristimulus values, distinguiamo il caso in cui si sta analizzando una radiazione/superficie emissiva
\begin{align}\label{chapter2:colorimetry:spectrum2XYZ4Source}
	X &= K\int_\Lambda L_{e,\Omega,\lambda}(\lambda)\bar{x}(\lambda)\mathrm{d}\lambda\\
	Y &= K\int_\Lambda L_{e,\Omega,\lambda}(\lambda)\bar{y}(\lambda)\mathrm{d}\lambda\\
	Z &= K\int_\Lambda L_{e,\Omega,\lambda}(\lambda)\bar{z}(\lambda)\mathrm{d}\lambda
\end{align}
Ed il caso in cui si sta analizzando un colore di una radiazione riflessa o trasmessa (fonti luminose secondarie)
\begin{align}
	X &= \frac{1}{\int_\Lambda L_{e,\Omega,\lambda}(\lambda)\bar{y}(\lambda)\mathrm{d}\lambda}
		\int_\Lambda S(\lambda)L_{e,\Omega,\lambda}(\lambda)\bar{x}(\lambda)\mathrm{d}\lambda\\
	Y &= \frac{1}{\int_\Lambda L_{e,\Omega,\lambda}(\lambda)\bar{y}(\lambda)\mathrm{d}\lambda}
		\int_\Lambda S(\lambda)L_{e,\Omega,\lambda}(\lambda)\bar{y}(\lambda)\mathrm{d}\lambda\\
	Z &= \frac{1}{\int_\Lambda L_{e,\Omega,\lambda}(\lambda)\bar{y}(\lambda)\mathrm{d}\lambda}
		\int_\Lambda S(\lambda)L_{e,\Omega,\lambda}(\lambda)\bar{z}(\lambda)\mathrm{d}\lambda\\
\end{align}
dove
\begin{equation}\label{chapter2:colorimetry:spectrum2XYZ4Rad}
	S(\lambda) = \left\{ \begin{aligned}
		R(\lambda)\;&\text{se riflessione}\\
		T(\lambda)\;&\text{se trasmissione}
	\end{aligned}\right.
\end{equation}
\subsection{xy chromaticity diagram}
\begin{figure}[tb]
	\includesvg[width=0.8\textwidth, inkscapelatex=false]{../assets/chapter2_colorimetry_chromaticity_diagram_1931.svg}
	\label{chapter2:colorimetry:cromaticityDiagram}
	\caption{xy Chromaticity Diagram del sistema tricromatico CIE XYZ 1931}
\end{figure}
Spesso si preferisce normalizzare i valori tristimolo per ottenere le \textit{coordinate di cromaticit\`a}
\begin{align}
	x = \frac{X}{X+Y+Z}\\
	y = \frac{Y}{X+Y+Z}\\
	z = \frac{Z}{X+Y+Z}
\end{align}
dove, in quanto $x+y+z=1$, soltanto le coordinate $xy$ sono necessarie per una completa descrizione del colore.\par
Il diagramma di cromaticit\`a \`e riportato in figura \ref{chapter2:colorimetry:cromaticityDiagram}. Il contorno curvo rappresenta l'insieme di punti
nel diagramma corrispondente alle radiazioni monocromatiche, mentre la curva evidenziata rappresenta il \textit{Luogo Planckiano}, insieme di punti
corrispondenti ad uno spettro di emissione di un corpo nero plankiano (formula \ref{chapter1:planckLaw}), con temperatura da $[1000,\infty]\,\si{K}$.
Parametro utile per descrivere ciascun punto del diagramma, in particolare quelli legati alle sorgenti luminose, \`e quello di
\begin{definitionS}
	La \textit{Temperatura di colore Correlata}(CCT) $T_{cp}$ \`e definita come temperatura del radiatore planckiano il cui colore percepito si
	avvicina di pi\`u allo stimolo dato nelle stesse condizioni di osservazione.\par
	Ci\`o si traduce nel trovare il punto nel luogo planckiano con distanza minima al punto di cromaticit\`a dato
\end{definitionS}
Altrettanto importanti nella specifica del diagramma di cromaticit\`a sono gli \textit{Illuminanti Standard}, sorgenti luminose teoriche con SPD e 
coordinate di cromaticit\`a note che approssimano determinate sorgenti luminose reali.
\begin{table}
	\begin{tabularx}{\linewidth}{cccY}
		\toprule
		Nome & CIE 1931 $2^o$ xy & CCT $\si{K}$ & Sorgente modellata\\
		\midrule
		A & $0.4476, 0.4075$ & $2856$ & Filamento di tungsteno incandescente\\
		D50 & $0.3457, 0.3585$ & $5003$ & Luce diurna all'orizzonte\\
		E & $\frac{1}{3}, \frac{1}{3}$ & $5454$ & SPD equienergia \\
		D55 & $0.3324, 0.3474$ & $5503$ & Luce diurna mattina/pomeriggio \\
		D65 & $0.3127, 0.3290$ & $6504$ & Luce diurna mezzogiorno\\
		\bottomrule
	\end{tabularx}
	\caption{Illuminanti standard CIE}
\end{table}
\subsection{sRGB color space}
Mentre per memorizzare image data un qualsiasi color space pu\`o essere utilizzato (infatti, si possono memorizzare anche dati relativi allo spettro 
direttamente [\cite{fichet}]), per mostrare colore a schermo si preferisce scegliere 3 primari ed un punto bianco, punto nel quale i 3 primari danno
contributo massimo. La scelta dei primari \`e guidata dalla percentuale di gamut che si desidera coprire con tutte le combinazioni lineari dei tre
primari scelti e limitazioni fisiche. Lo standard adottato per monitors e World Wide Web nel 1996 da IEC \`e \textit{sRGB}. Esso \`e un color space 
additivo basato sui tre primari rosso, verde, blu, e white point, le cui coordinate di cromaticit\`a sono riportate in tabella 
\ref{chapter2:colorimetry:sRGB}.\par
\begin{table}
	\begin{tabularx}{\linewidth}{ccccc}
		\toprule
		Cromaticit\`a & Rosso & Verde & Blu & White point(D65)\\
		\midrule
		$x$ & $0.64$ & $0.30$ & $0.15$ & $0.3127$\\
		$y$ & $0.33$ & $0.60$ & $0.06$ & $0.329$\\
		$Y$ & $0.2126$ & $0.7152$ & $0.0722$ & $1$\\
		\bottomrule
	\end{tabularx}
	\label{chapter2:colorimetry:sRGB}
	\caption{Coordinate degli stimoli primari e del white point del sRGB color space}
\end{table}
Altro componente per la specifica di un color space per un display \`e la funzione di trasferimento del display, in particolare 
\begin{definitionS}
	La \textit{electro-optical transfer function} (EOTF) \`e un funzione di trasferimento che converte un segnale immagine in input in intensit\`a 
	luminosa in output. Essa \textit{non \`e lineare}
\end{definitionS}
Tale funzione di trasferimento non \`e lineare per l'operazione di \textit{gamma correction} compiuta dai displays, la quale include sempre un 
elevamento a potenza $v_{in} = Av_{out}^\gamma$ (di solito $\gamma = 2.2$). Tale operazione \`e compiuta per ottimizzare l'uso dei bits 
nella codifica dell immagine, dando pi\`u importanza ai toni pi\`u scuri, in accordo con la percezione umana in grado di apprezzarli con 
pi\`u sensibilit\`a.\par
Dunque se l'output finale del sistema di rendering \`e un colore sRGB, dobbiamo essere capaci di convertire a/da un colore in sRGB color space da/a un
colore in XYZ color space. Sia $C_{srgb} = R_{srgb}|G_{srgb}|B_{srgb} \in [0,1]$ componenti del colore nello spazio sRGB gamma encoded. Simile
definizione per il colore nello spazio sRGB gamma corrected $C_{linear}$ ed il colore $[X, Y, Z]^T$ nello spazio XYZ.
\begin{equation}\label{chapter2:colorimetry:sRGB2XYZ}
	\text{Applica gamma expansion\footnotemark{}: }C_{linear} = \left\{\begin{alignedat}{2}
		&\frac{C_{srgb}}{12.92}, &C_{srgb}\leq 0.04045\\
		&\left(\frac{C_{srgb}+0.055}{1.055}\right)^{2.4},\;&C_{srgb}> 0.04045
	\end{alignedat}\right.
\end{equation}
\footnotetext{la semplice funzione potenza viene modificata secondo standards, come ITU-R per poter evitare problematiche. Per esempio, per evitare 
di avere derivata infinita nello zero, si definisce la funzione di gamma expansion per valori piccoli come divisione per una costante definita dallo 
standard}
\begin{equation}
	\text{Applica trasformazione lineare\footnotemark{}: }\begin{bmatrix}
		X_{D65}\\ Y_{D65}\\ Z_{D65}
	\end{bmatrix}=
	\begin{bmatrix}
		0.4124 & 0.3576 & 0.1805 \\
		0.2126 & 0.7152 & 0.0722 \\
		0.0193 & 0.1192 & 0.9505 
	\end{bmatrix}
	\begin{bmatrix}
		R_{linear} \\ G_{linear} \\ B_{linear}
	\end{bmatrix}
\end{equation}
\footnotetext{i valori tristimolo X,Y,Z qui utilizzati/ottenuti sono scalati in modo tale che l'illuminante standard D65 abbia luminanza unitaria, 
cio\`e moltiplicate per $\approx 3.039513678$}
\begin{figure}[tb]
	\label{chapter2:colorimetry:gamut}
	\includesvg[width=0.6\textwidth, inkscapelatex=false]{../assets/chapter2_colorimetry_CIE1931xy_gamut_comparison.svg}
	\caption{comparazione tra i diversi colorspaces, ed i colori percepibili che riescono a rappresentare. Notiamo che sRGB copre all'incirca $35\%$
	del color gamut}
\end{figure}
\subsection{Conversione da XYZ o RGB a SPD}
I color spaces sono spazi vettoriali, per i quali sono definite le operazioni di addizione tra colori (vettori) e moltiplicazione scalare colore.
\textit{La moltiplicazione tra pi\`u colori non \`e un operazione ben definita}. Nel trasporto della luce, la moltiplicazione tra due SPD \`e una 
operazione fondamentale. Si potrebbe pensare di definire arbitrariamente una operazione analoga in uno spazio RGB qualsiasi e definire la 
moltiplicazione tra due colori come \textit{prodotto di Hadamard} $\odot$.\par
Tale operazione risulta problematica perch\`e essa non \`e consistente tra color spaces, producendo risultati differenti (a volte al di fuori del 
color gamut, producendo un colore impossibile, si veda in figura \ref{chapter2:colorimetry:gamut} ProPhoto RGB), ma pi\`u importante, produce colori
eccessivamente pi\`u scuri e saturati.\par
Si giustifica dunque l'affermazione precedente di dover interpretare i colori in input negli arbitrari formati/color spaces supportati in uno spettro,
eseguire la computazione, ed infine convertire il risultato per ogni pixel in sRGB per display. Mentre la conversione 
\mbox{color space $\rightarrow$ SPD} \`e ben definita (vedi \ref{chapter2:colorimetry:spectrum2XYZ4Rad} e 
\ref{chapter2:colorimetry:spectrum2XYZ4Source}), non vale lo stesso per l'operazione inversa ed \`e ad oggi oggetto di ricerca [\cite{spectrum}].
Ci\`o che complica tale compito sono i requisiti di 
\begin{itemize}[topsep=0pt, noitemsep]
	\item[] \textit{Identit\`a} La conversione da RGB a spettro, seguita dalla ben definita operazione inversa, deve restituire lo stesso risultato
	\item[] \textit{Smoothness} Lo spettro ottenuto deve essere derivabile con continuit\`a affinch\`e, sotto nessune condizioni di luce, uno spettro
		di riflessione presenti seams visibili
	\item[] \textit{Conservazione dell'energia} Lo spettro ottenuto deve avere integrale $\in [0,1]$ se $[R, G, B]^T \in [0,1]^3$
\end{itemize}
L'approccio qui seguito \`e quello proposto da [\cite{pharr}] e [\cite{rgb2spec}], il quale consiste nel ricostruire un valore RGB utilizzando una
parabola parametrizzata da tre coefficienti $c_0,c_1,c_2$, la quale \`e prima resa limitata da una funzione sigmoide per poter ottenere un risultato
limitato in $[0,1]$. Vedi figura \ref{chapter2:colorimetry:Slambda}.
\begin{align}\label{chapter2:colorimetry:rgb2spec}
	S(\lambda) &= s(c_0\lambda^2 + c_1\lambda + c_2)\\
	s(x) &= \frac{1}{2} + \frac{x}{2\sqrt{1+x^2}}
\end{align}
\begin{figure}[tb]
	\begin{tikzpicture}
		\pgfmathdeclarefunction{sigmoid}{1}{\pgfmathparse{1/2+#1/(2*sqrt(1+#1^2))}}
		\begin{axis} [
			axis lines = left,
			domain=0:830,
			samples=100,
			xlabel = {$x$ [\si{nm}]},
			ylabel = $S(\lambda)$,
		]
			\addplot[draw=black]{sigmoid(-0.000073447*x^2+0.035354*x-2.607)};
		\end{axis}
	\end{tikzpicture}
	\caption{esempio di $S(\lambda)$ con $c_0 = -0.000073447,\;c_1 = 0.035354,\;c_2 = -2.607$}
	\label{chapter2:colorimetry:Slambda}
\end{figure}
Tali coefficienti sono calcolati a partire da un colore nell'sRGB color space, il quale \`e formulato come problema di ottimizzazione con scopo la
minimzzazione della metrica di residuo seguente
\begin{equation}\label{chapter2:colorimetry:rgb2specResidual}
	\begin{bmatrix}
		c_0^* \\ c_1^* \\ c_2^*
	\end{bmatrix}
	= \;\stackrel[c_0,c_1,c_2]{}{\mathrm{arg}\,\mathrm{min}}\norm{
		\begin{bmatrix}
			r \\ g \\ b
		\end{bmatrix}
		- \int_\Lambda 
		\begin{bmatrix}
			R(\lambda) \\ G(\lambda) \\ B(\lambda)
		\end{bmatrix}
		S(\lambda, c_0, c_1, c_2) W_{D_{65}}(\lambda)\mathrm{d}\lambda
	}
\end{equation}
dove $\norm{\cdot}$ \`e la CIE76 $\Delta E$ color distance\footnotemark{}
\begin{align*}
	\Delta E &= \sqrt{(L^*_2 - L^*_1)^2+(a^*_2 - a^*_1)^2+(b^*_2 - b^*_1)^2}\\
	\text{dove}\\
	f(x)  &= \left\{\begin{alignedat}{3}
			&\sqrt[3]{t} &\,&\text{se }\frac{Y}{Y_n}>\left(\frac{6}{29}\right)^3 \\
			&\frac{1}{3}\left(\frac{29}{6}\right)^2t+\frac{4}{29} \;\;&\,&\text{altrimenti}
		\end{alignedat}\right.\\
	L^* &= 16f\left(\frac{Y}{Y_n}\right)\\
	a^* &= 500\left(f\left(\frac{X}{X_n}\right)-f\left(\frac{Y}{Y_n}\right)\right)\\
	b^* &= 200\left(f\left(\frac{Y}{Y_n}\right)-f\left(\frac{Z}{Z_n}\right)\right)\\
\end{align*}
\footnotetext{Tale metrica richiede la conversione ad un color space uniforme, cio\`e dove la distanza fra colori \`e proporzionale alla differenza 
percepita dall'occhio umano. Il color space utilizzato nella metrica sopracitata \`e il CIE 1976 L*a*b*}
\begin{center}
con $X_n = 0.950489\;Y_n = 1\; Z_n = 1.08884$\\
se il bianco di riferimento \`e assunto come $Y_{white} = 1$\par
\end{center}
Tale problema di ottimizzazione \`e risolto con il metodo di Gauss-Newton (vedi \ref{appendixC:gaussNewton}).\par
Se questa operazione fosse eseguita per ogni colore sRGB nel processo di rendering, le performance ne risentirebbero. Dunque una lookup table 5D viene
precomputata. La struttura di tale tabella \`e dovuta al fatto che il gamut del sRGB color space \`e suddiviso in tre quadrilateri, tali che 
ciascuno di essi contenga rispettivamente 
\begin{itemize}[topsep=0pt, noitemsep]
	\item punti in cui $R = \max\{R, G, B\}$, corrispondente a indice 0 nella prima dimensione della LUT
	\item punti in cui $G = \max\{R, G, B\}$, corrispondente a indice 1 nella prima dimensione della LUT
	\item punti in cui $B = \max\{R, G, B\}$, corrispondente a indice 2 nella prima dimensione della LUT
\end{itemize}
Ciascuno di questi quadrilateri mappa a 4 cubi nello spazio RGB. Tali sottospazi sono campionati con una griglia uniforme 
$64\times 64\times64$ (le tre dimensioni 
delle 3 griglie costituiscono $2^a, 3^a, 4^a$ dimensioni della LUT). Per ciascuno di questi vengono calcolati i coefficienti del polinomio 
desiderati (i quali costituiscono l'ultima dimensione della LUT).\par
Nota aggiuntiva: spettri di illuminanti o di coefficienti che descrivono il mezzo trasmissivo tendono ad avere valori spettrali $>1$, il che significa
che bisogna gestire anche la conversione per coordinate RGB $>1$. Tali coordinate vengono normalizzate a $1$ o $0.5$ (a seconda della saturazione), 
affinch\`e si possa ottenere una migliore curva spettrale, e si memorizza un coefficiente di scala da utilizzare in seguito.
\section{camera}
Affinch\`e si possa associare ad ogni punto 2D nell'\textit{color buffer} che conterr\`a eventualmente l'immagine output uno o pi\`u coppie 
$(\vec{p_0}, \hat{\omega_0})$ punto e direzione di partenza, necessitiamo di un \textit{camera model}
\begin{definitionS}
	Un \textit{camera model} descrive la relazione matematica tra le coordinate 3D di un punto nella scena dal quale la luce proviene e le coordinate 
	2D della sua proiezione nel \textit{film plane}\footnotemark{} [\cite{vision}]
\end{definitionS}
\footnotetext{\textit{film} \`e il termine che utilizziamo per il piano dell'immagine finale [\cite{pharr}]}
\subsection{pinhole camera model}
\begin{figure}[tb]
	\includesvg[width=0.35\textwidth, inkscapelatex=false]{../assets/chapter2_camera_pinhole_diagram.svg}
	\includesvg[width=0.6\textwidth, inkscapelatex=false]{../assets/chapter2_camera_pinhole_plot.svg}
	\caption{A sinistra, diagramma del pinhole camera model. A destra, geometria del pinhole camera model con un sistema di riferimento left-handed.
	Di solito l'asse z punta verso la camera, risiedente in z=0}
\end{figure}
Modello di camera ideale secondo la quale la scena \`e osservata attraverso una apertura infinitesima. Tale modello genera immagini con messa a fuoco
nitida ovunque, trascurando tutte le distorsioni geometriche e effetti di aberrazione tipici delle lenti finite.\footnotemark{}\par
\footnotetext{descrivere ed implementare modelli di camera realistici \`e fuori dallo scope}
Considerazioni Geometriche:
% RIFAI
\begin{altDescription}{chapter2:camera:pinholeGeometry}
	\item[Origine] punto $\vec{o}$, nel quale \`e posizionata l'apertura della camera
	\item[Film plane] posizionato a distanza $n$ (f in figura), \textit{distanza focale} della camera, dall'origine $\vec{o}$. 
		Definisce il \textit{field of view}. Anche chiamato image plane o near plane
	\item[Punti] $\vec{p}$ e $\vec{q}$, rispettivamente punto da proiettare, "trasportante radianza", e punto $\vec{q}$, proiezione del punto $\vec{p}$
		sul film plane
\end{altDescription}
Definiamo inoltre dei Coordinate Spaces:
\begin{altDescription}{chapter2:camera:spaces}
	\item[Object Space] spazio tridimensionale in cui \`e definito ciascun oggetto
	\item[World Space] spazio tridimensionale utilizzato dalla scena
	\item[Camera Space] spazio tridimensionale avente come origine il punto di apertura della camera, asse x e y direzioni parallele al film plane,
		e z direzione perpendicolare al film plane
	\item[Rendering Space] spazio tridimensionale avente come origine il punto di apertura della camera, ma preservante le direzioni del world space.
		Lo utilizziamo negli algoritmi basati su ray tracing in quanto, fa in modo che gli oggetti vicini alla camera abbiano posizioni rappresentati
		da numeri piccoli, dunque sfruttanti precisione floating point maggiore, mentre le direzioni preservate permettono un migliore test di 
		intersezione mediante \textit{Axis Aligned Bounding Box}es
	\item[Image Space] spazio bidimensionale corrispondente a coordinate in $[-1,1]$ nel film plane, con origine il centro, e assi x, y paralleli alle 
		direzioni x,y del Camera Space
\end{altDescription}
Le formule che seguono utilizzano, per comodit\`a, il Camera Space.\par
\begin{figure}[tb]
	\begin{subfigure}{0.5\linewidth}
		\includesvg[width=\linewidth, inkscapelatex=false]{../assets/chapter2_camera_pinhole_projected.svg}
		\label{chapter2:camera:pinhole:pinhole_projected}
		\caption{Geometria del pinhole camera model visto dall'asse x}
	\end{subfigure}
	\begin{subfigure}{0.5\linewidth}
		\includegraphics[width=\linewidth, trim=4px 4px 100px 40px, clip]{../assets/chapter2_camera_non_linear_mapping_learnwebgl.png}
		\label{chapter2:camera:pinhole:learnWebGLzremap}
		\caption{Mostra la mappa nonlineare della depth\footnotemark{}}
	\end{subfigure}
	\caption{proiezione prospettica su pinhole camera model}
	\label{chapter2:camera:pinhole}
\end{figure}
\footnotetext{Immagine da \href{http://learnwebgl.brown37.net/08\_projections/projections\_perspective.html}
		{\mbox{http://learnwebgl.brown37.net/08\_projections/projections\_perspective.html}}}
La mappa $\vec{q}=[y_1,y_2]^T$ da $\vec{p}=[x_1,x_2,x_3]^T$. Siano $l,r,b,t,n,f$ rispettivamente left, right, bottom, top, near, far.\par
Possiamo ricavare una \textit{proiezione prospettica}\footnotemark{} esprimendo il punto $\vec{p}$ in coordinate omogenee 
e concatenando una trasformazione di 
\footnotetext{Weak perspective projection}
\textit{shearing}\footnotemark{} per sovrapporre far plane $[l,r]\times [b,t]$ con il near plane, seguito da una scala affinch\`e i due piani 
sovrapposti abbiano dimensione tale da ottenere un angolo di visione $90^o$.
\footnotetext{\href{https://en.wikipedia.org/wiki/Shear\_mapping}{https://en.wikipedia.org/wiki/Shear\_mapping}}
\begin{equation}\label{chapter2:camera:fromFrustum2nbynquad}
	\begin{bmatrix}
		\frac{2n}{r-l} & 0 & 0 & 0 \\ 0 & \frac{2n}{t-b} & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1
	\end{bmatrix}
	\begin{bmatrix}
		1 & 0 & -\frac{r+l}{2n} & 0 \\ 0 & 1 & -\frac{t+b}{2n} & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1
	\end{bmatrix}
	=
	\begin{bmatrix}
		\frac{2n}{r-l} & 0 & -\frac{r+l}{r-l} & 0 \\ 0 & \frac{2n}{t-b} & -\frac{t+b}{t-b} & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1
	\end{bmatrix}
\end{equation}
che mappa 
\begin{align*}
	[l, b, n, 1]^T &\mapsto [-n,-n, n, 1]^T \\
	[r, f, n, 1]^T &\mapsto [n, n, n, 1]^T
\end{align*}
Dunque il prossimo step \`e la normalizzazione delle tre coordinate spaziali, cio\`e la divisione di tutte le tre coordinate per la coordinata $z$,
affinch\`e pi\`u lontano sia un oggetto e pi\`u piccolo esso appare (step detto \textit{perspective divide}) il che \`e ottenuto ponendo un 
coefficiente non nullo nell'elemento di indice $(4,3)$ nella matrice di trasformazione. A seguire, centriamo la media armonica $\frac{2fn}{f+n}$
nell'origine e normalizziamo l'intervallo delle distanze, in modo tale che la mappa $[-n,f] \mapsto [-1,1]$ sia nonlineare, 
dando pi\`u importanza ai valori vicini alla camera. Vedi figura \ref{chapter2:camera:pinhole} \subref{chapter2:camera:pinhole:learnWebGLzremap}
\begin{equation}\label{chapter2:camera:remapDepthNonlinear}
	\begin{bmatrix}
		1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & \frac{2}{f-n} & 0 \\ 0 & 0 & 0 & 1
	\end{bmatrix}
	\begin{bmatrix}
		1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & \frac{f+n}{2} & -fn \\ 0 & 0 & 1 & 0
	\end{bmatrix}
	=
	\begin{bmatrix}
		1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & \frac{f+n}{f-n} & -\frac{2fn}{f-n} \\ 0 & 0 & 1 & 0
	\end{bmatrix}
\end{equation}
Combinando \ref{chapter2:camera:remapDepthNonlinear} e \ref{chapter2:camera:fromFrustum2nbynquad} si ottiene una trasformazione che mappa il view
frustum, piramide retta a base piramidale con dimensioni arbitrarie, ad un cubo $[-1,1]^3$
\begin{equation}\label{chapter2:camera:perspectiveMatrix}
	\begin{bmatrix}
		1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & \frac{f+n}{f-n} & -\frac{2fn}{f-n} \\ 0 & 0 & 1 & 0
	\end{bmatrix}
	\begin{bmatrix}
		\frac{2n}{r-l} & 0 & -\frac{r+l}{r-l} & 0 \\ 0 & \frac{2n}{t-b} & -\frac{t+b}{t-b} & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1
	\end{bmatrix}
	=
	\begin{bmatrix}
		\frac{2n}{r-l} & 0 & -\frac{r+l}{r-l} & 0 \\ 0 & \frac{2n}{t-b} & -\frac{t+b}{t-b} & 0 \\ 0 & 0 & \frac{f+n}{f-n} & -\frac{2fn}{f-n} 
		\\ 0 & 0 & 1 & 0
	\end{bmatrix}
\end{equation}
\subsection{filtrare image samples}
Dopo il campionamento di un pixel, e il calcolo della radianza spettrale in ogni pixel sample, bisogna aggregare tutti i contributi di ogni campione.
Tutti i contributi dovrebbero avere un peso inversamente proporzionale alla distanza dal centro. Ci\`o suggerisce l'uso di un 
\textit{filtro passa basso} $f(x-x',y-y')$, dove $(x,y)$ coordinate in cui si calcola la \textit{filtered image function} 
$r_f(x,y)$ e $(x',y')$ displacement.\par
Idealmente, per ottenere la migliore stima possibile, si dovrebbero estrarre $\infty$ samples ed integrarli nella pixel cell area $A_{px}$
\begin{equation}
	r_f(x,y) = \int_{A_{px}} f(x-x', y-y')r(x',y') \mathrm{d}A
\end{equation}
Come meglio trattato in capitolo \ref{chapter6}, lo \textit{stimatore} di monte carlo ci permette di stimare tale integrale come
\begin{equation}
	r_f(x,y) \approx \frac{1}{n}\sum_{i=1}^n \frac{f(x-x_i, y-y_i)r(x_i, y_i)}{p(x_i, y_i)}
\end{equation}
se consideriamo il sample point come vettore aleatorio con distribuzione uniforme, allora l'equazione diventa
\begin{equation}\label{chapter2:camera:unbiasedEstimator}
	r_f(x,y) \approx \frac{\norm{A_{px}}}{n}\sum_{i=1}^n f(x-x_i, y-y_i)r(x_i, y_i)
\end{equation}
Equazione \ref{chapter2:camera:unbiasedEstimator} pu\`o essere migliorata in termini di varianza pesando ciascun campione affinch\`e la somma dei 
valori filtrati sia pari a $1$ anche con campioni finiti (la sua aspettazione \`e 1). Tale stimatore \`e detto \textit{weighted importance sampling
Monte Carlo estimator}
\begin{equation}\label{chapter2:camera:weightEstimatorAll}
	r_f(x,y) \approx \frac{\sum_{i=1}^n f(x-x_i,y-y_i)r(x_i,y_i)}{\sum_{i=1}^n f(x-x_i,y-y_i)}
\end{equation}
Tali formule hanno il difetto di richiedere tutti i campioni nel film plane per stimare la radianza spettrale in un singolo pixel. La soluzione consiste
nel scegliere opportunamente un filtro che lascia passare solo i campioni all'interno del pixel stesso, e campionare la distribuzione di samples con
una distribuzione simile alla filter function. Tale approccio \`e nominato \textit{filter importance sampling} (vedi \ref{chapter6}).
\begin{align*}
	r_f(x,y) &\approx \frac{1}{n}\sum_{i=1}^n \left(\frac{f(x-x_i, y-y_i)}{p(x_i, y_i)} r(x_i, y_i)\right)\\
			&= \frac{\left(\displaystyle{\int_{\mathbb{R}^2}} \vert f(x',y')\vert \mathrm{d}A\right)}{n}\sum_{i=1}^n \mathrm{sgn}(f(x-x_i,y-y_i))r(x_i,y_i)
\end{align*}
Combinando tale approccio con \textit{weighted importance sampling}
\begin{equation}\label{chapter2:camera:weightEstimatorOne}
	r_f(x,y) \approx \frac{\sum_{i=1}^n w(x-x_i, y-y_i)r(x_i,y_i)}{\sum_{i=1}^n w(x-x_i, y-y_i)}
\end{equation}
con \[w=\frac{f(x,y)}{p(x,y)}\].\par
Con il primo approccio, \ref{chapter2:camera:weightEstimatorAll}, il vantaggio \`e quello di calcolare una stima per ogni pixel basata su un numero 
maggiore di campioni distribuiti su tutto il film plane. Tale approccio ha i suoi svantaggi nella performance, e nel fatto che se i campioni sono 
ben distribuiti una tale ricostruzione non \`e necessaria, rendendo \ref{chapter2:camera:weightEstimatorOne} un buon approccio in questo caso.
